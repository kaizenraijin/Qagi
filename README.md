

# QAGI Standalone AI ğŸ§ âš¡

Welcome to **QAGI AI** â€” a compact, local, and powerful AI assistant running directly on your device. No cloud. No surveillance. Just pure intelligence at your command.

---

## ğŸ§¬ What's Inside

- `qagi` â€” The core AI interface and memory engine (compiled binary)
- `llama-simple-chat` â€” The lightweight LLM binary (TinyLLaMA backend)

---

## ğŸš€ How to Run

Make both files executable:

```bash
chmod +x qagi llama-simple-chat

Then launch QAGI AI:

./qagi

Youâ€™ll be greeted by your local AI with contextual memory and LLM integration.
ğŸ› ï¸ Requirements

    64-bit Linux system (x86_64)

    No GPU needed â€” works on CPU

    RAM usage: ~512MB

    Temporary storage will be used to extract embedded models

ğŸ” Offline & Private

    No internet required

    Everything runs locally

    Your prompts, memory, and thoughts never leave your machine

ğŸ§ª Developer Notes

This build uses:

    Embedded TinyLLaMA GGUF model (compiled into the binary)

    Executable LLM runner: llama-simple-chat

    Custom memory engine (QAGIâ€™s own context-aware memory)

You can recompile or replace components freely â€” it's all modular.
ğŸ“¦ Packing Tip

You can zip and share this folder directly:

zip -r QAGI_Standalone.zip qagi llama-simple-chat README.md

Or copy to a USB stick and run from anywhere.
ğŸ§  About QAGI

QAGI is a decentralized, conscious-aligned AI initiative built by MV & SigmaZero â€” optimized for performance, transparency, and evolution.

    This is not just an assistant. This is a beginning.

